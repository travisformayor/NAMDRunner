use crate::types::*;
use anyhow::{anyhow, Result};

/// SLURM script generator for NAMD molecular dynamics jobs
pub struct SlurmScriptGenerator;

impl SlurmScriptGenerator {
    /// Generate a complete SLURM batch script for a NAMD job
    pub fn generate_namd_script(job_info: &JobInfo) -> Result<String> {
        // Validate inputs before script generation
        Self::validate_job_info(job_info)?;

        // Extract configuration details
        // Sanitize job name for SLURM (replace spaces/special chars with underscores)
        let job_name = Self::sanitize_slurm_job_name(&job_info.job_name);
        let slurm_config = &job_info.slurm_config;
        let _namd_config = &job_info.namd_config;

        // Ensure memory has unit suffix (GB or MB) - critical for correct allocation
        // SLURM bare numbers are in MB (64 = 64MB not 64GB!)
        let memory_with_unit = if slurm_config.memory.contains("GB") || slurm_config.memory.contains("MB") {
            slurm_config.memory.clone()
        } else {
            // Assume GB if no unit specified (most common case)
            format!("{}GB", slurm_config.memory)
        };

        // Get scratch directory for working directory
        let working_dir = job_info.scratch_dir
            .as_ref()
            .ok_or_else(|| anyhow!("Job scratch directory not configured"))?;

        // Generate the SLURM script content
        let script = format!(
r#"#!/bin/bash
#SBATCH --job-name={}
#SBATCH --output={}_%j.out
#SBATCH --error={}_%j.err
#SBATCH --partition=amilan
#SBATCH --nodes=1
#SBATCH --ntasks={}
#SBATCH --time={}
#SBATCH --mem={}
#SBATCH --qos=normal
#SBATCH --constraint=ib

# Generated by NAMDRunner on {}
# Job ID: {}

# Initialize module environment
source /etc/profile
export SLURM_EXPORT_ENV=ALL  # Required for OpenMPI

# Load required modules for NAMD execution
module purge
module load gcc/14.2.0
module load openmpi/5.0.6
module load namd/3.0.1_cpu

# Change to working directory
cd {}

# Execute NAMD with MPI (OpenMPI handles CPU affinity automatically)
mpirun -np $SLURM_NTASKS namd3 scripts/config.namd > namd_output.log
"#,
            job_name,
            job_name,
            job_name,
            slurm_config.cores,
            slurm_config.walltime,
            memory_with_unit,
            chrono::Utc::now().to_rfc3339(),
            job_info.job_id,
            working_dir
        );

        Ok(script)
    }

    /// Generate NAMD configuration file content
    pub fn generate_namd_config(job_info: &JobInfo) -> Result<String> {
        let namd_config = &job_info.namd_config;

        // Extract actual uploaded file names by type (NOT hardcoded filenames!)
        // PSF file (required)
        let psf_file = job_info.input_files.iter()
            .find(|f| matches!(f.file_type, Some(NAMDFileType::Psf)))
            .ok_or_else(|| anyhow!("No PSF file found in input files"))?;

        // PDB file (required)
        let pdb_file = job_info.input_files.iter()
            .find(|f| matches!(f.file_type, Some(NAMDFileType::Pdb)))
            .ok_or_else(|| anyhow!("No PDB file found in input files"))?;

        // Parameter files (at least one required)
        let param_files: Vec<_> = job_info.input_files.iter()
            .filter(|f| matches!(f.file_type, Some(NAMDFileType::Prm)))
            .collect();

        if param_files.is_empty() {
            return Err(anyhow!("No parameter files found in input files"));
        }

        // Use JobDirectoryStructure for consistent path references with ACTUAL filenames
        let structure_path = crate::ssh::JobDirectoryStructure::input_path(&psf_file.name);
        let coordinates_path = crate::ssh::JobDirectoryStructure::input_path(&pdb_file.name);
        let output_path = crate::ssh::JobDirectoryStructure::output_path(&namd_config.outputname);

        // Generate parameter file lines dynamically
        let param_lines: Vec<String> = param_files.iter()
            .map(|f| format!("parameters         {}", crate::ssh::JobDirectoryStructure::input_path(&f.name)))
            .collect();

        // Generate basic NAMD configuration based on the template from reference docs
        let config = format!(
r#"#############################################################
## JOB DESCRIPTION                                         ##
#############################################################
# {}
# Generated by NAMDRunner on {}
# Job ID: {}

#############################################################
## ADJUSTABLE PARAMETERS                                   ##
#############################################################

# Input structure and coordinates
structure          {}
coordinates        {}

# Output naming
outputName         {}
binaryoutput       yes

# Temperature for this simulation
set temperature    {}

# Starting fresh simulation
temperature        $temperature
firsttimestep      0

#############################################################
## SIMULATION PARAMETERS                                   ##
#############################################################

# Force field parameters
paraTypeCharmm     on
{}

# Non-bonded force calculations
exclude            scaled1-4
1-4scaling         1.0
switching          on
switchdist         8
cutoff             10
pairlistdist       12

# Integration parameters
timestep           {}
rigidBonds         all
nonbondedFreq      1
fullElectFrequency 2
stepspercycle      12

# PME (Particle Mesh Ewald) for electrostatics
PME                yes
PMEGridSpacing     1.5

# Temperature control (Langevin dynamics)
langevin           on
langevinDamping    5
langevinTemp       $temperature
langevinHydrogen   off

# Pressure control (NPT ensemble)
langevinPiston        on
langevinPistonTarget  1.01325
langevinPistonPeriod  1000
langevinPistonDecay   500
langevinPistonTemp    $temperature

# Output frequencies (in timesteps)
xstFreq            {}
outputEnergies     {}
dcdfreq            {}
restartfreq        {}
outputPressure     {}

# Trajectory wrapping
wrapAll            off
wrapWater          off

#############################################################
## EXECUTION SCRIPT                                        ##
#############################################################

# Production run
run {}
"#,
            job_info.job_name,
            chrono::Utc::now().to_rfc3339(),
            job_info.job_id,
            structure_path,
            coordinates_path,
            output_path,
            namd_config.temperature,
            param_lines.join("\n"),
            namd_config.timestep,
            namd_config.dcd_freq.unwrap_or(9600),
            namd_config.dcd_freq.unwrap_or(9600),
            namd_config.dcd_freq.unwrap_or(9600),
            namd_config.restart_freq.unwrap_or(9600),
            namd_config.dcd_freq.unwrap_or(9600),
            namd_config.steps
        );

        Ok(config)
    }

    /// Sanitize job name for use in SLURM job-name directive
    /// Replaces spaces and special characters with underscores, keeps alphanumeric and hyphens
    fn sanitize_slurm_job_name(name: &str) -> String {
        name.chars()
            .map(|c| {
                if c.is_alphanumeric() || c == '-' {
                    c
                } else {
                    '_'
                }
            })
            .collect()
    }

    /// Validate job information before script generation
    fn validate_job_info(job_info: &JobInfo) -> Result<()> {
        // Validate job name
        if job_info.job_name.trim().is_empty() {
            return Err(anyhow!("Job name cannot be empty"));
        }

        // Validate SLURM configuration
        let slurm_config = &job_info.slurm_config;
        if slurm_config.cores < 1 {
            return Err(anyhow!("Core count must be at least 1"));
        }

        // Phase 6 constraint: Single-node only (nodes=1 always in script)
        // Alpine amilan nodes have 64 cores, so enforce this limit
        if slurm_config.cores > 64 {
            return Err(anyhow!("Core count cannot exceed 64 (single node limit for amilan partition)"));
        }

        if slurm_config.walltime.is_empty() {
            return Err(anyhow!("Walltime cannot be empty"));
        }

        if slurm_config.memory.is_empty() {
            return Err(anyhow!("Memory specification cannot be empty"));
        }

        // Validate NAMD configuration
        let namd_config = &job_info.namd_config;
        if namd_config.steps < 1 {
            return Err(anyhow!("Simulation steps must be positive"));
        }

        if namd_config.temperature < 200.0 || namd_config.temperature > 400.0 {
            return Err(anyhow!("Temperature {} K is outside biological range (200-400 K)", namd_config.temperature));
        }

        if namd_config.timestep < 0.1 || namd_config.timestep > 4.0 {
            return Err(anyhow!("Timestep {} fs is outside safe range (0.1-4.0 fs)", namd_config.timestep));
        }

        // Validate scratch directory is configured
        if job_info.scratch_dir.is_none() {
            return Err(anyhow!("Scratch directory not configured for job"));
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn create_test_job() -> JobInfo {
        JobInfo {
            job_id: "test_job_001".to_string(),
            job_name: "Test NAMD Job".to_string(),
            namd_config: NAMDConfig {
                steps: 100000,
                temperature: 300.0,
                timestep: 2.0,
                outputname: "test_output".to_string(),
                dcd_freq: Some(1000),
                restart_freq: Some(5000),
            },
            slurm_config: SlurmConfig {
                cores: 24,
                memory: "32GB".to_string(),
                walltime: "04:00:00".to_string(),
                partition: Some("amilan".to_string()),
                qos: None,
            },
            input_files: vec![
                InputFile {
                    name: "structure.psf".to_string(),
                    local_path: "/tmp/structure.psf".to_string(),
                    remote_name: Some("structure.psf".to_string()),
                    file_type: Some(NAMDFileType::Psf),
                    size: Some(1024),
                    uploaded_at: Some(chrono::Utc::now().to_rfc3339()),
                },
                InputFile {
                    name: "structure.pdb".to_string(),
                    local_path: "/tmp/structure.pdb".to_string(),
                    remote_name: Some("structure.pdb".to_string()),
                    file_type: Some(NAMDFileType::Pdb),
                    size: Some(2048),
                    uploaded_at: Some(chrono::Utc::now().to_rfc3339()),
                },
                InputFile {
                    name: "par_all36_na.prm".to_string(),
                    local_path: "/tmp/par_all36_na.prm".to_string(),
                    remote_name: Some("par_all36_na.prm".to_string()),
                    file_type: Some(NAMDFileType::Prm),
                    size: Some(512),
                    uploaded_at: Some(chrono::Utc::now().to_rfc3339()),
                },
                InputFile {
                    name: "par_water_ions_cufix.prm".to_string(),
                    local_path: "/tmp/par_water_ions_cufix.prm".to_string(),
                    remote_name: Some("par_water_ions_cufix.prm".to_string()),
                    file_type: Some(NAMDFileType::Prm),
                    size: Some(256),
                    uploaded_at: Some(chrono::Utc::now().to_rfc3339()),
                },
            ],
            output_files: None,
            project_dir: Some("/projects/testuser/namdrunner_jobs/test_job_001".to_string()),
            scratch_dir: Some("/scratch/alpine/testuser/namdrunner_jobs/test_job_001".to_string()),
            status: JobStatus::Created,
            created_at: chrono::Utc::now().to_rfc3339(),
            updated_at: None,
            submitted_at: None,
            completed_at: None,
            slurm_job_id: None,
            error_info: None,
            slurm_stdout: None,
            slurm_stderr: None,
            remote_directory: "/projects/testuser/namdrunner_jobs/test_job_001".to_string(),
        }
    }

    #[test]
    fn test_generate_slurm_script() {
        let job = create_test_job();
        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();

        // Verify essential SLURM directives
        assert!(script.contains("#SBATCH --job-name=Test_NAMD_Job"));
        assert!(script.contains("#SBATCH --ntasks=24"));
        assert!(script.contains("#SBATCH --time=04:00:00"));
        assert!(script.contains("#SBATCH --mem=32GB"));
        assert!(script.contains("#SBATCH --partition=amilan"));

        // Verify module loading
        assert!(script.contains("module purge"));
        assert!(script.contains("module load gcc/14.2.0"));
        assert!(script.contains("module load openmpi/5.0.6"));
        assert!(script.contains("module load namd/3.0.1_cpu"));

        // Verify NAMD execution
        assert!(script.contains("mpirun -np $SLURM_NTASKS namd3"));
        assert!(!script.contains("+setcpuaffinity"), "Script should not contain +setcpuaffinity flag");
        assert!(!script.contains("+pemap"), "Script should not contain +pemap flag");
        assert!(script.contains("scripts/config.namd > namd_output.log"));

        // Verify working directory
        assert!(script.contains("cd /scratch/alpine/testuser/namdrunner_jobs/test_job_001"));
    }

    #[test]
    fn test_generate_namd_config() {
        let job = create_test_job();
        let config = SlurmScriptGenerator::generate_namd_config(&job).unwrap();

        // Verify essential NAMD parameters
        assert!(config.contains("structure          input_files/structure.psf"));
        assert!(config.contains("coordinates        input_files/structure.pdb"));
        assert!(config.contains("outputName         outputs/test_output"));
        assert!(config.contains("set temperature    300"));
        assert!(config.contains("timestep           2"));
        assert!(config.contains("run 100000"));

        // Verify force field setup
        assert!(config.contains("paraTypeCharmm     on"));
        assert!(config.contains("parameters         input_files/par_all36_na.prm"));

        // Verify output frequencies
        assert!(config.contains("dcdfreq            1000"));
        assert!(config.contains("restartfreq        5000"));
    }

    #[test]
    fn test_validation_empty_job_name() {
        let mut job = create_test_job();
        job.job_name = "".to_string();

        let result = SlurmScriptGenerator::generate_namd_script(&job);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Job name cannot be empty"));
    }

    // Note: Cluster-specific validation (e.g., max cores, memory limits) is performed
    // by validation::job_validation before job creation, not in the script generator.
    // The script generator only validates that required fields are present and non-empty.

    #[test]
    fn test_validation_invalid_temperature() {
        let mut job = create_test_job();
        job.namd_config.temperature = 100.0; // Too cold

        let result = SlurmScriptGenerator::generate_namd_script(&job);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Temperature"));
    }

    #[test]
    fn test_memory_unit_fix_adds_gb_suffix() {
        // Test that memory without unit gets GB suffix added
        let mut job = create_test_job();
        job.slurm_config.memory = "64".to_string(); // No unit

        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();

        // Should have GB suffix added
        assert!(script.contains("#SBATCH --mem=64GB"), "Memory should have GB suffix added");
        assert!(!script.contains("#SBATCH --mem=64\n"), "Memory should not be bare number");
    }

    #[test]
    fn test_memory_unit_preserves_existing_gb() {
        // Test that memory with GB is preserved
        let mut job = create_test_job();
        job.slurm_config.memory = "32GB".to_string();

        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();

        assert!(script.contains("#SBATCH --mem=32GB"), "Memory with GB should be preserved");
    }

    #[test]
    fn test_memory_unit_preserves_mb() {
        // Test that memory with MB is preserved
        let mut job = create_test_job();
        job.slurm_config.memory = "1024MB".to_string();

        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();

        assert!(script.contains("#SBATCH --mem=1024MB"), "Memory with MB should be preserved");
    }

    #[test]
    fn test_namd_config_uses_actual_uploaded_filenames() {
        // Test that NAMD config uses actual uploaded file names, not hardcoded
        let job = create_test_job();
        let config = SlurmScriptGenerator::generate_namd_config(&job).unwrap();

        // Should use actual filenames from input_files
        assert!(config.contains("structure          input_files/structure.psf"));
        assert!(config.contains("coordinates        input_files/structure.pdb"));
        assert!(config.contains("parameters         input_files/par_all36_na.prm"));
        assert!(config.contains("parameters         input_files/par_water_ions_cufix.prm"));

        // Should NOT contain hardcoded generic names if they differ
        // (This test verifies the implementation extracts from input_files)
    }

    #[test]
    fn test_openmpi_environment_export_present() {
        // Test that OpenMPI environment export is in script
        let job = create_test_job();
        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();

        assert!(script.contains("export SLURM_EXPORT_ENV=ALL"),
            "Script should contain OpenMPI environment export");

        // Verify it's after profile source and before module commands
        let profile_pos = script.find("source /etc/profile").unwrap();
        let export_pos = script.find("export SLURM_EXPORT_ENV=ALL").unwrap();
        let module_pos = script.find("module purge").unwrap();

        assert!(export_pos > profile_pos, "Export should be after profile source");
        assert!(export_pos < module_pos, "Export should be before module commands");
    }

    #[test]
    fn test_single_node_validation() {
        // Test that cores > 64 is rejected (single node constraint)
        let mut job = create_test_job();
        job.slurm_config.cores = 65; // Exceeds single node limit

        let result = SlurmScriptGenerator::generate_namd_script(&job);
        assert!(result.is_err(), "Should reject cores > 64");
        assert!(result.unwrap_err().to_string().contains("64"),
            "Error should mention 64 core limit");
    }

    #[test]
    fn test_nodes_always_one_for_valid_cores() {
        // Test that --nodes=1 for valid core counts
        let mut job = create_test_job();
        job.slurm_config.cores = 32;

        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();

        assert!(script.contains("#SBATCH --nodes=1"),
            "Should always use --nodes=1 for single-node jobs");
        assert!(script.contains("#SBATCH --ntasks=32"),
            "Should use correct core count");
    }

    #[test]
    fn test_validation_missing_scratch_dir() {
        let mut job = create_test_job();
        job.scratch_dir = None;

        let result = SlurmScriptGenerator::generate_namd_script(&job);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Scratch directory not configured"));
    }

    #[test]
    fn test_job_name_sanitization() {
        let mut job = create_test_job();
        job.job_name = "Test Job With Spaces".to_string();

        let script = SlurmScriptGenerator::generate_namd_script(&job).unwrap();
        assert!(script.contains("#SBATCH --job-name=Test_Job_With_Spaces"));
    }
}