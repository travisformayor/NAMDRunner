# Database & Data Schemas

This document defines all data persistence patterns for NAMDRunner, including SQLite storage, JSON metadata formats, validation rules, and data management strategies.

## Table of Contents
- [Database Architecture](#database-architecture)
- [SQLite Schema](#sqlite-schema)
- [JSON Metadata Schema](#json-metadata-schema)
  - [job_info.json (Server-Side)](#job_infojson-server-side)
  - [InputFile Schema](#inputfile-schema)
  - [OutputFile Schema](#outputfile-schema)
- [File Organization](#file-organization)
- [Validation Rules](#validation-rules)
- [Data Type Mappings](#data-type-mappings)
- [Best Practices](#best-practices)

## Database Architecture

**Design Philosophy**: Simple document store for job caching. No complex schema, no migrations, no manual serialization.

### Key Principles
1. **Document Storage**: Entire `JobInfo` struct serialized as JSON
2. **No Schema Coupling**: Adding fields to Rust struct = automatic DB support via serde
3. **Zero Migrations**: No backward compatibility needed - users can delete old DB
4. **Performance**: SQLite operations are fast enough for desktop use (< 100 jobs typical)

### When Data is Stored
- **Local SQLite**: Job caching only - enables offline viewing of job list
- **Cluster (job_info.json)**: Single source of truth for job metadata
- **Sync Pattern**: Download from cluster → cache in SQLite → display in UI

## SQLite Schema

**Extremely simple - just two columns:**

```sql
-- Simple document store for job caching
CREATE TABLE IF NOT EXISTS jobs (
    job_id TEXT PRIMARY KEY,
    data TEXT NOT NULL
);

-- Index on status for filtering (uses JSON extraction)
CREATE INDEX IF NOT EXISTS idx_jobs_status
ON jobs(json_extract(data, '$.status'));
```

### Why This Works
- **No manual column mapping** - serde handles everything
- **No column index hell** - just one JSON blob
- **Easy to extend** - add field to Rust type, done
- **JSON functions** - SQLite can query JSON directly for the status index

### API Usage

```rust
// Save entire job
db.save_job(&job_info)?;

// Load job
let job = db.load_job("job_001")?;

// Load all jobs (sorted by created_at DESC)
let jobs = db.load_all_jobs()?;

// Delete job
db.delete_job("job_001")?;
```

**That's it.** No manual serialization, no column lists, no migrations.

## JSON Metadata Schema

### job_info.json (Server-Side)
This file is created in each job directory on the cluster and contains all job metadata. The schema is generated by serializing the `JobInfo` struct from `src-tauri/src/types/core.rs`.

```json
{
  "job_id": "test-1_1760733363035078",
  "job_name": "test-1",
  "status": "COMPLETED",
  "slurm_job_id": "12345678",
  "created_at": "2025-01-15T10:30:00Z",
  "updated_at": "2025-01-15T11:45:00Z",
  "submitted_at": "2025-01-15T10:35:00Z",
  "completed_at": "2025-01-15T11:00:00Z",
  "project_dir": "/projects/username/namdrunner_jobs/test-1_1760733363035078",
  "scratch_dir": "/scratch/alpine/username/namdrunner_jobs/test-1_1760733363035078",
  "error_info": null,
  "slurm_stdout": null,
  "slurm_stderr": null,
  "namd_config": {
    "steps": 50000,
    "temperature": 310.0,
    "timestep": 2.0,
    "outputname": "output",
    "dcd_freq": 1000,
    "restart_freq": 5000
  },
  "slurm_config": {
    "cores": 24,
    "memory": "16GB",
    "walltime": "02:00:00",
    "partition": "amilan",
    "qos": "normal"
  },
  "input_files": [
    {
      "name": "hextube.pdb",
      "local_path": "/home/user/Documents/hextube.pdb",
      "remote_name": "hextube.pdb",
      "file_type": "pdb",
      "size": 2209942,
      "uploaded_at": "2025-01-15T10:32:00Z"
    },
    {
      "name": "hextube.psf",
      "local_path": "/home/user/Documents/hextube.psf",
      "remote_name": "hextube.psf",
      "file_type": "psf",
      "size": 6732288,
      "uploaded_at": "2025-01-15T10:32:05Z"
    },
    {
      "name": "par_all36_na.prm",
      "local_path": "/home/user/Documents/par_all36_na.prm",
      "remote_name": "par_all36_na.prm",
      "file_type": "prm",
      "size": 61066,
      "uploaded_at": "2025-01-15T10:32:07Z"
    }
  ],
  "output_files": [
    {
      "name": "output.dcd",
      "size": 145829376,
      "modified_at": "2025-01-15T11:00:00Z"
    },
    {
      "name": "restart.coor",
      "size": 8192000,
      "modified_at": "2025-01-15T11:00:00Z"
    },
    {
      "name": "restart.vel",
      "size": 8192000,
      "modified_at": "2025-01-15T11:00:00Z"
    },
    {
      "name": "restart.xsc",
      "size": 4096,
      "modified_at": "2025-01-15T11:00:00Z"
    }
  ],
  "remote_directory": "/projects/username/namdrunner_jobs/test-1_1760733363035078"
}
```

### InputFile Schema

```typescript
interface InputFile {
  name: string;              // Display filename
  local_path: string;        // Local path when uploading (not used after upload)
  remote_name?: string;      // Actual filename on server
  file_type?: 'pdb' | 'psf' | 'prm' | 'other';  // NAMD file type
  size?: number;             // File size in bytes (populated after upload)
  uploaded_at?: string;      // RFC3339 timestamp when uploaded
}
```

**When populated:**
- `size` and `uploaded_at`: Set during job creation after each file upload completes
- Backend stats the remote file after successful SFTP transfer
- Allows UI to display file sizes without re-querying server

### OutputFile Schema

```typescript
interface OutputFile {
  name: string;              // Filename (e.g., "output.dcd")
  size: number;              // File size in bytes
  modified_at: string;       // RFC3339 timestamp from server
}
```

**When populated:**
- Created during automatic job completion (when job reaches terminal state)
- Backend does single batch SFTP readdir in project directory's output_files/
- All output files queried at once (no per-file round trips)

## File Organization

### Directory Structure
```
/projects/$USER/namdrunner_jobs/
└── {job_id}/
    ├── job_info.json           # Complete job metadata (this schema)
    ├── input_files/
    │   ├── structure.pdb
    │   ├── structure.psf
    │   └── parameters.prm
    ├── scripts/
    │   ├── config.namd         # Generated NAMD config
    │   └── job.sbatch          # Generated SLURM script
    ├── output_files/           # Created after job completion (rsync from scratch)
    │   ├── output.dcd          # Trajectory
    │   ├── restart.coor        # Restart files
    │   ├── restart.vel
    │   └── restart.xsc
    └── logs/                   # SLURM logs
        ├── {job_name}_{slurm_job_id}.out
        └── {job_name}_{slurm_job_id}.err

/scratch/alpine/$USER/namdrunner_jobs/
└── {job_id}/                   # Working directory during execution
    ├── config.namd             # Symlinked/copied from project
    ├── job.sbatch
    ├── structure.pdb
    ├── structure.psf
    ├── parameters.prm
    ├── namd_output.log
    ├── output.dcd              # Generated during run
    ├── restart.coor
    ├── restart.vel
    └── restart.xsc
```

**Key Points:**
- **Project directory**: Permanent storage, survives job completion
- **Scratch directory**: Fast local storage during job run, auto-purged after 90 days
- **Automatic rsync**: On job completion, scratch → project (data preservation)
- **Metadata fetching**: Input file sizes fetched after upload, output file sizes fetched after job completion

### File Naming Conventions
- **SLURM script**: `scripts/job.sbatch`
- **NAMD config**: `scripts/config.namd`
- **Job metadata**: `job_info.json` (in job root)
- **SLURM Stdout**: `logs/{job_name}_{slurm_job_id}.out`
- **SLURM Stderr**: `logs/{job_name}_{slurm_job_id}.err`
- **Trajectory**: `output_files/output.dcd`
- **Restart files**: `output_files/restart.{coor,vel,xsc}`

## Validation Rules

### Job ID Format
- **Pattern**: `{job_name}_{timestamp_millis}`
- **Examples**: `test-1_1760733363035078`, `sim_alpha_1760040523960286`
- **Globally unique** (timestamp component ensures uniqueness)

### File Path Validation
- **No directory traversal** (`../`)
- **Allowed file extensions**: `.pdb`, `.psf`, `.prm`, `.namd`, `.sbatch`, `.out`, `.err`, `.log`, `.dcd`, `.coor`, `.vel`, `.xsc`
- **Size limits**: 1GB per file (configurable)

### Parameter Ranges
```typescript
interface ValidationRules {
  namd: {
    steps: { min: 1, max: 100000000 };
    temperature: { min: 200, max: 400 };  // Kelvin
    timestep: { min: 0.1, max: 4.0 };     // femtoseconds
  };
  slurm: {
    cores: { min: 1, max: 64 };           // Single-node limit
    memory_gb: { min: 1, max: 256 };      // Varies by partition
    walltime_hours: { min: 0.1, max: 168 }; // Up to 7 days with long QoS
  };
}
```

## Data Type Mappings

### TypeScript ↔ Rust
```rust
// Core types
pub type JobId = String;          // {name}_{timestamp}
pub type SlurmJobId = String;     // SLURM's numeric job ID
pub type Timestamp = String;      // RFC3339 format

// Status enum (serde handles serialization)
#[derive(Serialize, Deserialize)]
pub enum JobStatus {
    #[serde(rename = "CREATED")]
    Created,
    #[serde(rename = "PENDING")]
    Pending,
    #[serde(rename = "RUNNING")]
    Running,
    #[serde(rename = "COMPLETED")]
    Completed,
    #[serde(rename = "FAILED")]
    Failed,
    #[serde(rename = "CANCELLED")]
    Cancelled,
}

// File type enum
#[derive(Serialize, Deserialize)]
pub enum NAMDFileType {
    #[serde(rename = "pdb")]
    Pdb,
    #[serde(rename = "psf")]
    Psf,
    #[serde(rename = "prm")]
    Prm,
    #[serde(rename = "other")]
    Other,
}
```

### SQLite JSON ↔ Rust
```rust
// Save: serde_json::to_string(&job_info)
// Load: serde_json::from_str::<JobInfo>(&json_data)
```

**No manual mapping needed** - serde handles everything automatically.

## Best Practices

### Database Operations
1. **Always use `with_database()` wrapper** for access
2. **Synchronous is fine** - SQLite operations are microseconds, no async overhead needed
3. **No connection pooling** - single-threaded desktop app, one connection is enough
4. **Graceful degradation** - old DB? Delete it, recreate with new schema

### Schema Changes
1. **No migrations** - users delete old DB file
2. **Add fields freely** - serde handles missing fields with `#[serde(default)]`
3. **Breaking changes OK** - development phase, no backward compatibility burden

### Performance
- **Typical dataset**: ~100 jobs cached locally
- **Query time**: < 1ms for load_all_jobs()
- **JSON parsing**: Negligible overhead for this data size
- **Status index**: Fast filtering by job status

### Data Integrity
1. **Server is source of truth** - SQLite is just a cache
2. **Sync resolves conflicts** - download from server overwrites local
3. **No complex sync logic** - simple replace-on-sync pattern
4. **User can always re-sync** - if local DB corrupt, just sync again

---

*For IPC interfaces and API contracts, see [`docs/API.md`](API.md).*
*For cluster-specific file system details, see [`docs/reference/alpine-cluster-reference.md`](reference/alpine-cluster-reference.md).*
